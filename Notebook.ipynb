{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Super Resolution \n",
    "Sparse super-resolution entails post-processing images of few sufficiently separated light sources to resolve their positions to a higher accuracy than that of the imaging device.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figs/start2.png' width='800'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine that the four lines the first image represent four point-light sources distributed in a two dimensional space, with the the height of each representing its intensity.  The diffraction limit of light, and the lens through which an observation is made, blur the light signal that is emitted from the four sources, so the combined signal might look like the middle image.  Capturing and storing an image of the light involves discretization.  If the four light sources are located very close to each other, it may occur that all four sources fall within a 6 by 6 pixel square, resulting the given image.\n",
    "\n",
    "The sparse super resolution problem is, given such an image, a model of the *point spread function* (PSF), and the assumption that there are 'few' sources, find the locations and intensities of the light sources.\n",
    "\n",
    "Let $k$ light sources be located at points $t_i \\in \\Omega$, where $\\Omega$ is a spatial domain bounded by the extremal measurements.  Each source has a positive amplitude $a_i$, so that the object may be modelled by a discrete measure, $$z = \\sum_{i = 1}^{k} a_i \\delta_{t_i}.$$ \n",
    "\n",
    "The measurement device is characterised by $m$ continuous functions $\\phi_j$, which aim to approximate the measurement device's PSF at a particular location $j$ so that measurements are given by the convolution,\n",
    "\n",
    "\\begin{align*}\n",
    "y_j = \\int_{\\Omega} \\phi_j(t)z(dt) = \\sum_{i=1}^k a_i \\phi_j(t_i), \\ j = 1, 2, \\dots m.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "Writing $ \\textbf{y} = [y_1, y_2, \\dots y_m]^T$, $\\Phi = [\\phi_1(t), \\phi_2(t), \\dots \\phi_m(t)]^T$, the localisation problem is to find a sparse measure $\\hat{z} \\geq 0$, from the measurements $\\textbf{y}$, which is achieved by solving,\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimise}\\quad\\left\\|\\textbf{y} - \\int_{\\Omega} \\Phi(t) \\hat{z} ({\\rm d}t) \\right\\|_2,\n",
    "\\end{align*}\n",
    "\n",
    "subject to the support of $z$ in $\\Omega$.  This may be rewritten as,\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimise}\\quad\\left\\|\\textbf{y} - \\hat{\\Phi}\\hat{\\textbf{a}} \\right\\|_2,\n",
    "\\end{align*}\n",
    "\n",
    "where the hat notation is used to denote estimates of the values charactering the object of interest and $\\hat{\\Phi} \\in \\mathbb{R}^{m\\times \\hat{k}}$, $\\{\\hat{\\Phi}\\}_{ji} = \\phi_j(\\hat{t}_i)$, $\\hat{\\textbf{a}} \\in \\mathbb{R}^{\\hat{k}}$.  One approach to solving this problem is via a *Nonlinear Least Squares* (NLS) formulation. Let $\\Omega \\equiv [0, 1] \\times [0, 1]$, and with normalisation we may assume  $0 \\leq a_i \\leq 1$.  Then the problem is \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimise}&\\quad f^*(x) = \\frac{1}{2} \\left\\| r(x) \\right\\|_{2}^2,  \\\\ \n",
    "\\text{s.t.}&\\quad0 \\leq x_i \\leq 1, \\quad i=1, 2, \\dots \\hat{k},\n",
    "\\end{align*}\n",
    "\n",
    "where $r(x) = \\textbf{y} - \\hat{\\Phi}\\hat{\\textbf{a}}$,  $x = [t_{1,1} \\dots t_{\\hat{k},1}, t_{1, 2}, \\dots t_{\\hat{k}, 2}, a_1, \\dots a_{\\hat{k}} ]. $  Converting the contraints to a penalty function and considering the unconstrained NLS,\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{minimise}&\\quad f(x) = \\frac{1}{2} \\left\\| r(x) \\right\\|_2^2 + \\frac{\\alpha}{2} \\left\\| c(x) \\right\\|_2^2 ,\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "\\begin{align*}\n",
    "\\{c(x)\\}_i = \\begin{cases}\n",
    "x_i - b_u &\\text{ if } x_i > b_u,\\\\\n",
    "b_l - x_i &\\text{ if } x_i < b_l, \\\\\n",
    "\\ 0 &\\text{ otherwise,}\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "$b_l = 0,\\ b_u = 1$, allows the use of classic NLS algorithms for unconstainted optimisation.  For example, the *Gauss-Newton* (GN) method performs updates using steps $s$ which are solutions to,\n",
    "\n",
    "\\begin{align*}\n",
    "\\mbox{minimise}&\\quad f_m(s) = f(x) + \\nabla f(x)^T s + \\frac{1}{2}s^T B s, \n",
    "\\end{align*}\n",
    "\n",
    "where $B =  J_1(x)^TJ_1(x) + \\alpha J_2(x)^TJ_2(x)$, $\\nabla f(x) = J(x)_1 ^ T r(x) + \\alpha J_2(x)^T c(x)$, $J_1 \\in \\mathbb{R}^{m\\, \\times\\, \\hat{k}(D + 1)}$ the Jacobian of $r$ and $J_2 \\in \\mathbb{R}^{\\hat{k}(D + 1)\\, \\times\\, \\hat{k}(D + 1)}$ the Jacobian of $c$.  \n",
    "\n",
    "A popular alteration of GN is the Levenberg-Marquardt method where a damping parameter $\\gamma$ is added to the hessian model $B = J^T J + \\gamma I$, which promotes smaller steps and improves conditioning, thereby improving the convergence properties.  \n",
    "\n",
    "In the above,\n",
    "\n",
    "\\begin{align*}\n",
    "\\{J_1(x)\\}_{jw} &= \\frac{\\partial \\phi_j}{\\partial \\hat{t}_{id}} \\hat{a}_i, \\quad j = 1, 2, \\dots m, \\quad i = 1, 2, \\dots \\hat{k}, \\quad d = 1, 2 \\quad w = i + (d - 1) \\cdot \\hat{k},\\\\\n",
    "\\{J_1(x)\\}_{jw}  &= \\phi_j(t_i), \\quad j = 1, 2, \\dots m, \\quad i = 1, 2, \\dots \\hat{k}, \\quad w = i + 2\\hat{k},\n",
    "\\end{align*}\n",
    "\n",
    "and, \n",
    "\n",
    "\\begin{align*}\n",
    "\\{J_2(x)\\}_{ii}  &= \\begin{cases}\n",
    "1 &\\text{ if } x_i > b_u,\\\\\n",
    "-1 &\\text{ if } x_i < b_l, \\\\\n",
    "\\ 0 &\\text{ otherwise,}\n",
    "\\end{cases}\\\\\n",
    "\\{J_2(x)\\}_{ij} &= 0,\\quad i \\neq j,\n",
    "\\end{align*}\n",
    "for $i = 1, 2, \\dots \\hat{k}, \\quad j = 1, 2, \\dots \\hat{k}$.\n",
    "\n",
    "\n",
    "Assuming a Gaussian PSF,  \n",
    "\n",
    "\\begin{align*}\n",
    "\\phi_{j}(t) &= \\text{exp}\\left({-\\frac{\\|t - s_{j}\\|_2^2}{\\sigma^2}}\\right), \\\\\n",
    "\\frac{\\partial \\phi_j}{\\partial t_{id}} &= \\frac{2(t_{id} - s_{jd})}{\\sigma^2} \\phi_{j}(t)\n",
    "\\end{align*}\n",
    "\n",
    "where $s_j$ is the spatial position of measurement $j$.\n",
    "\n",
    "So far, the fact that the number of sources $\\hat{k}$ is unknown has not been addressed.  A simple approach, inspired by the [ADCG method](https://arxiv.org/pdf/1507.01562.pdf), is to first insert the source which would result in the greatest decrease in the objective.  Then, the PSF-signal of that source is subtracted from the original image (thresholding negative values), to obtain a residual-image.  The subsequent source insertion is done by finding the maximal decrease over the residual-image.  After each insertion, the sources are collectively adjusted until a new minimum is found, after which is residual-image is updated.  This is repeated until the objective reaches a termination tolerance.\n",
    "\n",
    "INITIALISE: \n",
    "\n",
    "$x = \\{\\}$, $y = y*$\n",
    "\n",
    "DO:\n",
    "1. minimise $f(x_{\\text{new}})$ for $y^*$ and append $x_{\\text{new}}$ to $x$\n",
    "2. minimise $f(x)$ for measurement $y$\n",
    "3. update $y^* = \\max(0, y - \\hat{\\Phi}\\hat{a})$\n",
    "\n",
    "WHILE: $f(x) >$ tolerance and iteration limit not reached.\n",
    "\n",
    "A rudimentary implementation of the Levenberg-Marquardt method is used to find search directions in (1) and (2), with a newton linesearch to find a step size.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='figs/sr4_2.gif' width='800'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
